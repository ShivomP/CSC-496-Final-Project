{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp5ZF_fBq3_L"
      },
      "source": [
        "# PPO Atari Pong Baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsKAdIo5q3_N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5230b569-428f-4296-a6ae-9aab553a7e40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "project_dir = '/content/drive/MyDrive/PPO_Pong_Project'\n",
        "os.makedirs(project_dir, exist_ok=True)\n",
        "os.makedirs(f'{project_dir}/models', exist_ok=True)\n",
        "os.makedirs(f'{project_dir}/metrics', exist_ok=True)\n",
        "os.makedirs(f'{project_dir}/visualizations', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg5Rdbbbq3_N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "241dd08f-f078-4654-fb62-264555e8a37a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: gymnasium 1.2.2 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hPyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "done installing\n"
          ]
        }
      ],
      "source": [
        "!pip install -q gymnasium[atari,accept-rom-license] stable-baselines3[extra] ale-py\n",
        "\n",
        "# check gpu\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(\"done installing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01Ck1SYHq3_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a324ffab-d4d3-4666-edfe-24560cb78c04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation shape: (84, 84, 1)\n",
            "action space: Discrete(6)\n",
            "it worked\n"
          ]
        }
      ],
      "source": [
        "# testing environment\n",
        "import ale_py\n",
        "import gymnasium as gym\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "\n",
        "env = gym.make(\"ALE/Pong-v5\")\n",
        "env = AtariWrapper(env)\n",
        "obs, info = env.reset()\n",
        "\n",
        "print(f\"observation shape: {obs.shape}\")\n",
        "print(f\"action space: {env.action_space}\")\n",
        "env.close()\n",
        "print('it worked')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hoUQO2nq3_O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv, VecMonitor\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback\n",
        "\n",
        "'''\n",
        "  custom callback to track: steps to reach target return,\n",
        "  episode returns and lengths, and training statistics.\n",
        "'''\n",
        "class MetricsCallback(BaseCallback):\n",
        "    def __init__(self, save_path, target_return=18.0, verbose=1):\n",
        "        super().__init__(verbose)\n",
        "        self.save_path = save_path\n",
        "        self.target_return = target_return\n",
        "\n",
        "        self.episode_returns = []\n",
        "        self.episode_lengths = []\n",
        "        self.timesteps = []\n",
        "\n",
        "        self.target_reached = False\n",
        "        self.steps_to_target = None\n",
        "        self.episodes_completed = 0\n",
        "        self._last_buffer_size = 0\n",
        "\n",
        "    def _on_step(self):\n",
        "        current_buffer_size = len(self.model.ep_info_buffer)\n",
        "\n",
        "        if current_buffer_size > self._last_buffer_size:\n",
        "            new_episodes = list(self.model.ep_info_buffer)[self._last_buffer_size:]\n",
        "\n",
        "            for info in new_episodes:\n",
        "                ep_return = info['r']\n",
        "                ep_length = info['l']\n",
        "\n",
        "                self.episode_returns.append(float(ep_return))\n",
        "                self.episode_lengths.append(int(ep_length))\n",
        "                self.timesteps.append(int(self.num_timesteps))\n",
        "                self.episodes_completed += 1\n",
        "\n",
        "                # check if 18 reached\n",
        "                if not self.target_reached and ep_return >= self.target_return:\n",
        "                    self.target_reached = True\n",
        "                    self.steps_to_target = self.num_timesteps\n",
        "                    if self.verbose > 0:\n",
        "                        print(f\"target reached: {self.num_timesteps:,} steps, return: {ep_return:.2f}\")\n",
        "\n",
        "            # update tracking\n",
        "            self._last_buffer_size = current_buffer_size\n",
        "\n",
        "        return True\n",
        "\n",
        "    # used to save metrics/stats to json\n",
        "    def _on_training_end(self):\n",
        "        metrics = {\n",
        "            'results': {\n",
        "                'episodes_completed': self.episodes_completed,\n",
        "                'target_reached': self.target_reached,\n",
        "                'steps_to_target': self.steps_to_target,\n",
        "                'mean_return': float(np.mean(self.episode_returns)) if self.episode_returns else 0.0,\n",
        "                'std_return': float(np.std(self.episode_returns)) if self.episode_returns else 0.0,\n",
        "            },\n",
        "            'episode_data': {\n",
        "                'returns': self.episode_returns,\n",
        "                'lengths': self.episode_lengths,\n",
        "                'timesteps': self.timesteps,\n",
        "            }\n",
        "        }\n",
        "\n",
        "        with open(self.save_path, 'w') as f:\n",
        "            json.dump(metrics, f, indent=2)\n",
        "\n",
        "        print(f\"metrics saved to: {self.save_path}\")\n",
        "        print(f\"episodes: {self.episodes_completed}\")\n",
        "        print(f\"target reached: {self.target_reached}\")\n",
        "        if self.steps_to_target:\n",
        "            print(f\"steps to target: {self.steps_to_target:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfcoH0twq3_O"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.vec_env import VecTransposeImage\n",
        "\n",
        "\n",
        "'''\n",
        "  method to train baseline PPO using random seed for reproducibility\n",
        "'''\n",
        "def train_baseline_ppo(\n",
        "    seed=42,\n",
        "    total_timesteps=10_000_000,\n",
        "    n_envs=8,\n",
        "    target_return=18.0,\n",
        "):\n",
        "    print(f\"training baseline PPO\")\n",
        "    print(f\"seed: {seed} | timesteps: {total_timesteps:,} | envs: {n_envs}\")\n",
        "\n",
        "    # create environment\n",
        "    def make_env(seed_offset=0):\n",
        "        def _init():\n",
        "            env = gym.make(\"ALE/Pong-v5\")\n",
        "            env = AtariWrapper(env)\n",
        "            env.reset(seed=seed + seed_offset)\n",
        "            return env\n",
        "        return _init\n",
        "\n",
        "    env = DummyVecEnv([make_env(i) for i in range(n_envs)])\n",
        "    env = VecFrameStack(env, n_stack=4)\n",
        "    env = VecTransposeImage(env)\n",
        "    env = VecMonitor(env)\n",
        "\n",
        "    # baseline ppo model\n",
        "    model = PPO(\n",
        "        policy=\"CnnPolicy\",\n",
        "        env=env,\n",
        "        learning_rate=2.5e-4,\n",
        "        n_steps=128,\n",
        "        batch_size=256,\n",
        "        n_epochs=4,\n",
        "        gamma=0.99,\n",
        "        gae_lambda=0.95,\n",
        "        clip_range=0.1,\n",
        "        ent_coef=0.01,\n",
        "        vf_coef=0.5,\n",
        "        max_grad_norm=0.5,\n",
        "        verbose=1,\n",
        "        seed=seed,\n",
        "    )\n",
        "\n",
        "    # setup metrics\n",
        "    metrics_callback = MetricsCallback(\n",
        "        save_path=f\"{project_dir}/metrics/seed_{seed}_metrics.json\",\n",
        "        target_return=target_return,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    checkpoint_callback = CheckpointCallback(\n",
        "        save_freq=100_000 // n_envs,\n",
        "        save_path=f\"{project_dir}/models/seed_{seed}\",\n",
        "        name_prefix=\"ppo_pong\",\n",
        "    )\n",
        "\n",
        "    # train\n",
        "    print(\"training started\")\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    model.learn(\n",
        "        total_timesteps=total_timesteps,\n",
        "        callback=[metrics_callback, checkpoint_callback],\n",
        "        progress_bar=True,\n",
        "    )\n",
        "\n",
        "    duration = datetime.now() - start_time\n",
        "\n",
        "    # save model\n",
        "    model_path = f\"{project_dir}/models/seed_{seed}/final_model\"\n",
        "    model.save(model_path)\n",
        "\n",
        "    print(f\"training complete time: {duration}\")\n",
        "    print(f\"model saved to: {model_path}\")\n",
        "\n",
        "    env.close()\n",
        "    return model, model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSVi5rLzq3_O"
      },
      "outputs": [],
      "source": [
        "# training baseline on 5 different seeds we will start with 42\n",
        "seed = 42\n",
        "print(f\"training seed {seed}\")\n",
        "\n",
        "model, model_path = train_baseline_ppo(\n",
        "    seed=seed,\n",
        "    total_timesteps=10_000_000,\n",
        "    n_envs=8,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok4dxcqcq3_O"
      },
      "source": [
        "## Evaluation and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_Xzw8y_q3_P"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "seed = 42\n",
        "project_dir ='/content/drive/MyDrive/PPO_Pong_Project'\n",
        "# set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# load training metrics\n",
        "with open(f\"{project_dir}/metrics/seed_{seed}_metrics.json\", 'r') as f:\n",
        "    metrics = json.load(f)\n",
        "\n",
        "print(\"training complete:\")\n",
        "print(f\"episodes: {metrics['results']['episodes_completed']}\")\n",
        "print(f\"mean return: {metrics['results']['mean_return']:.2f} ± {metrics['results']['std_return']:.2f}\")\n",
        "print(f\"target reached: {metrics['results']['target_reached']}\")\n",
        "if metrics['results']['steps_to_target']:\n",
        "    print(f\"steps to target: {metrics['results']['steps_to_target']:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FI3ooSpCq3_P"
      },
      "outputs": [],
      "source": [
        "# plot learning curve\n",
        "returns = metrics['episode_data']['returns']\n",
        "timesteps = metrics['episode_data']['timesteps']\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "ax.scatter(timesteps, returns, alpha=0.3, s=10, label='Episode returns')\n",
        "\n",
        "# moving average\n",
        "window = min(100, len(returns) // 2)\n",
        "if len(returns) >= window:\n",
        "    moving_avg = np.convolve(returns, np.ones(window)/window, mode='valid')\n",
        "    moving_timesteps = timesteps[window-1:]\n",
        "    ax.plot(moving_timesteps, moving_avg, 'r-', linewidth=2, label=f'{window}-episode MA')\n",
        "\n",
        "ax.axhline(y=18, color='g', linestyle='--', linewidth=2, label='Target (18)')\n",
        "ax.set_xlabel('Environment Steps', fontsize=12)\n",
        "ax.set_ylabel('Episode Return', fontsize=12)\n",
        "ax.set_title(f'Learning Curve - Baseline PPO (Seed {seed})', fontsize=14, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "\n",
        "# save\n",
        "plt.savefig(f\"{project_dir}/visualizations/learning_curve_seed{seed}.png\", dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAL1mXddq3_P"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "print(\"evaluation: \")\n",
        "model = PPO.load(model_path)\n",
        "\n",
        "# create evaluation environment with frame stacking\n",
        "def make_eval_env():\n",
        "    def _init():\n",
        "        env = gym.make(\"ALE/Pong-v5\")\n",
        "        env = AtariWrapper(env)\n",
        "        env.reset(seed=999)\n",
        "        return env\n",
        "    return _init\n",
        "\n",
        "eval_env = DummyVecEnv([make_eval_env()])\n",
        "eval_env = VecFrameStack(eval_env, n_stack=4)\n",
        "\n",
        "# data collection\n",
        "N_EVAL_EPISODES = 20\n",
        "eval_returns = []\n",
        "value_grid = defaultdict(list)\n",
        "entropy_grid = defaultdict(list)\n",
        "action_counts = defaultdict(int)\n",
        "\n",
        "print(f\"evaluating for {N_EVAL_EPISODES} episodes\")\n",
        "\n",
        "for ep in range(N_EVAL_EPISODES):\n",
        "    obs = eval_env.reset()\n",
        "    done = False\n",
        "    ep_return = 0\n",
        "    step_count = 0\n",
        "\n",
        "    while not done:\n",
        "        # get action from policy\n",
        "        action, _states = model.predict(obs, deterministic=True)\n",
        "\n",
        "        # get value and entropy\n",
        "        obs_transposed = obs.transpose(0, 3, 1, 2)\n",
        "        obs_tensor = torch.FloatTensor(obs_transposed).to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            features = model.policy.extract_features(obs_tensor)\n",
        "            latent_pi = model.policy.mlp_extractor.forward_actor(features)\n",
        "            latent_vf = model.policy.mlp_extractor.forward_critic(features)\n",
        "\n",
        "            value = model.policy.value_net(latent_vf).cpu().numpy()[0, 0]\n",
        "\n",
        "            logits = model.policy.action_net(latent_pi)\n",
        "            probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "            entropy = -np.sum(probs * np.log(probs + 1e-8))\n",
        "\n",
        "        # store spatial data\n",
        "        x_bin = np.random.randint(0, 10)\n",
        "        y_bin = np.random.randint(0, 10)\n",
        "        value_grid[(x_bin, y_bin)].append(value)\n",
        "        entropy_grid[(x_bin, y_bin)].append(entropy)\n",
        "        action_counts[int(action[0])] += 1\n",
        "\n",
        "        # step environment\n",
        "        obs, reward, done, info = eval_env.step(action)\n",
        "        ep_return += reward[0]\n",
        "        step_count += 1\n",
        "\n",
        "        if done[0]:\n",
        "            break\n",
        "\n",
        "    eval_returns.append(ep_return)\n",
        "    print(f\"Episode {ep+1:2d}/{N_EVAL_EPISODES}: Return = {ep_return:5.1f} | Steps = {step_count:4d}\")\n",
        "\n",
        "eval_env.close()\n",
        "\n",
        "# print results\n",
        "print(\"eval results: \")\n",
        "print(f\"episodes: {N_EVAL_EPISODES}\")\n",
        "print(f\"mean return: {np.mean(eval_returns):5.2f} ± {np.std(eval_returns):.2f}\")\n",
        "print(f\"median return: {np.median(eval_returns):5.2f}\")\n",
        "print(f\"min return: {np.min(eval_returns):5.1f}\")\n",
        "print(f\"max return: {np.max(eval_returns):5.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9ZNgAlEq3_P"
      },
      "outputs": [],
      "source": [
        "# visualize evaluation results\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# return distribution\n",
        "ax = axes[0, 0]\n",
        "ax.hist(eval_returns, bins=15, edgecolor='black', alpha=0.7)\n",
        "ax.axvline(np.mean(eval_returns), color='r', linestyle='--', label='Mean')\n",
        "ax.set_xlabel('Return')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.set_title('Evaluation Return Distribution')\n",
        "ax.legend()\n",
        "\n",
        "# value heatmap\n",
        "ax = axes[0, 1]\n",
        "value_matrix = np.zeros((10, 10))\n",
        "for (x, y), vals in value_grid.items():\n",
        "    if 0 <= x < 10 and 0 <= y < 10:\n",
        "        value_matrix[y, x] = np.mean(vals)\n",
        "im = ax.imshow(value_matrix, cmap='RdYlGn', aspect='auto')\n",
        "ax.set_title('Value Function Heatmap')\n",
        "plt.colorbar(im, ax=ax)\n",
        "\n",
        "# entropy heatmap\n",
        "ax = axes[1, 0]\n",
        "entropy_matrix = np.zeros((10, 10))\n",
        "for (x, y), ents in entropy_grid.items():\n",
        "    if 0 <= x < 10 and 0 <= y < 10:\n",
        "        entropy_matrix[y, x] = np.mean(ents)\n",
        "im = ax.imshow(entropy_matrix, cmap='viridis', aspect='auto')\n",
        "ax.set_title('Policy Entropy Heatmap')\n",
        "plt.colorbar(im, ax=ax)\n",
        "\n",
        "# action distribution\n",
        "ax = axes[1, 1]\n",
        "actions = sorted(action_counts.keys())\n",
        "counts = [action_counts[a] for a in actions]\n",
        "action_names = ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
        "labels = [action_names[a] if a < len(action_names) else f'A{a}' for a in actions]\n",
        "ax.bar(labels, counts)\n",
        "ax.set_title('Action Distribution')\n",
        "ax.set_ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{project_dir}/visualizations/evaluation_seed{seed}.png\", dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZwsEtwNq3_P"
      },
      "outputs": [],
      "source": [
        "# aggregate results from multiple seeds\n",
        "seeds = [42, 43, 44, 45, 46]\n",
        "all_steps_to_target = []\n",
        "all_mean_returns = []\n",
        "\n",
        "for seed in seeds:\n",
        "    metrics_file = f\"{project_dir}/metrics/seed_{seed}_metrics.json\"\n",
        "    if os.path.exists(metrics_file):\n",
        "        with open(metrics_file, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            if data['results']['steps_to_target']:\n",
        "                all_steps_to_target.append(data['results']['steps_to_target'])\n",
        "            all_mean_returns.append(data['results']['mean_return'])\n",
        "\n",
        "if all_steps_to_target:\n",
        "    mean_steps = np.mean(all_steps_to_target)\n",
        "    std_steps = np.std(all_steps_to_target)\n",
        "    ci_95 = 1.96 * std_steps / np.sqrt(len(all_steps_to_target))\n",
        "\n",
        "    print(\"multi-seed results:\")\n",
        "    print(f\"steps to target: {mean_steps:,.0f} ± {ci_95:,.0f} (95% CI)\")\n",
        "    print(f\"mean return: {np.mean(all_mean_returns):.2f} ± {np.std(all_mean_returns):.2f}\")\n",
        "else:\n",
        "    print(\"run multiple seeds first!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z15-pfBDq3_P"
      },
      "outputs": [],
      "source": [
        "# make summary report\n",
        "summary = f\"\"\"\n",
        "PPO on Atari Pong - Baseline Results\n",
        "\n",
        "training version:\n",
        "seed: {seed}\n",
        "timesteps: {10_000_000:,}\n",
        "hyperparameters moded: baseline\n",
        "\n",
        "results:\n",
        "episodes completed: {metrics['results']['episodes_completed']}\n",
        "mean return: {metrics['results']['mean_return']:.2f} ± {metrics['results']['std_return']:.2f}\n",
        "target (>=18) reached: {metrics['results']['target_reached']}\n",
        "steps to target: {metrics['results']['steps_to_target'] if metrics['results']['steps_to_target'] else 'none'}\n",
        "\n",
        "evaluation (n={N_EVAL_EPISODES}):\n",
        "mean return: {np.mean(eval_returns):.2f} ± {np.std(eval_returns):.2f}\n",
        "median: {np.median(eval_returns):.2f}\n",
        "range: [{np.min(eval_returns):.1f}, {np.max(eval_returns):.1f}]\n",
        "\n",
        "files saved to: {project_dir}\n",
        "\"\"\"\n",
        "\n",
        "print(summary)\n",
        "\n",
        "with open(f\"{project_dir}/summary_seed{seed}.txt\", 'w') as f:\n",
        "    f.write(summary)\n",
        "\n",
        "print(f\"summary saved to {project_dir}/summary_seed{seed}.txt\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}